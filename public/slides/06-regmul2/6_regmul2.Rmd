---
title: "Estadística Multivariada"
author: ".small[Juan Carlos Castillo <br><br> Departamento de Sociología - UCH / COES <br><br>]"
date: "2do Sem 2019"
output:
  xaringan::moon_reader:
    css: ["../custom_2020.css","https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.7.0/animate.min.css"] # "../ninjutsu.css", paraflipbook
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      beforeInit: "https://multinivel.netlify.com/docpres/xaringan_custom/macros.js"
    seal: false # esto omite title slide automática
---
class: front

```{r eval=FALSE, include=FALSE}
# Correr esto para que funcione el infinite moonreader, el root folder debe ser static para si dirigir solo "bajndo" en directorios hacia el bib y otros

xaringan::inf_mr('/static/docpres/02_bases/2mlmbases.Rmd')

o en RStudio:
  - abrir desde carpeta root del proyecto
  - Addins-> infinite moon reader
```


```{r setup, include=FALSE, cache = FALSE}
require("knitr")
options(htmltools.dir.version = FALSE)
pacman::p_load(RefManageR)
# bib <- ReadBib("../../bib/electivomultinivel.bib", check = FALSE)
opts_chunk$set(warning=FALSE,
             message=FALSE,
             echo=FALSE,
             cache = TRUE, fig.width=7, fig.height=5.2)
library(flipbookr)
library(tidyverse)
```

<!---
Para correr en ATOM
- open terminal, abrir R (simplemente, R y enter)
- rmarkdown::render('static/docpres/07_interacciones/7interacciones.Rmd', 'xaringan::moon_reader')

About macros.js: permite escalar las imágenes como [scale 50%](path to image), hay si que grabar ese archivo js en el directorio.
--->


.pull-left[
# Estadística Multivariada
## Juan Carlos Castillo
## Sociología FACSO - UChile
## 1er Sem 2020
## [multivariada.netlify.com](https://multivariada.netlify.com)
]


.pull-right[
.right[
![:scale 70%](https://multivariada.netlify.com/img/hex_multiva.png)
<br>
<br>
## Sesión 6: Regresión múltiple (2)
]

]
---

layout: true
class: animated, fadeIn


---
class: inverse, bottom, right, animated, slideInRight



# **Contenidos**

## 1. Repaso de sesión anterior

## 2. Bases de control y parcialización

## 3. Demostración parcialización

---
class: roja, middle, right

# 1. Repaso sesión anterior

---

.pull-left-narrow[

## Base: Modelo de regresión (simple)

<br>
$$\widehat{Y}=b_{0} +b_{1}X$$ 

.center[
![:scale 100%](../images/regmod.png)]
]

.pull-right-wide[

- Se estima mediante el método de mínimos cuadrados ordinarios (OLS)

- Permite estimar el valor de una variable ( $\widehat{Y}$ ) a partir del valor conocido de otra variable ( $X$ )

- La estimación se expresa en el coeficiente de regresión $b_{1}$, también llamado "beta" o pendiente

- Este coeficiente se interpreta de la siguiente manera: Por cada unidad que aumenta X, Y aumenta en $b_{1}$ unidades
]

---
# Estadística multivariada

- Hechos sociales: **multicausales**

![:scale 55%](../05-regmul1/multicausal.png)

---
# Regresión múltiple: más de 1 predictor

$$\widehat{Y}=b_{0}+b_{1}X_{1}+b_{2}X_{2}+b_{3}X_{3}+...+b_{k}X_{k}$$

--

.center[![:scale 50%](../images/regmul.png)]

---
# Regresión múltiple: más de 1 predictor

- es un modelo **SUMATIVO**

$$\widehat{Y}=b_{0}+b_{1}X_{1}+b_{2}X_{2}+b_{3}X_{3}+...+b_{k}X_{k}$$

- para que esta suma tenga sentido, se deben descontar los aportes "duplicados" o que se traslapan entre predictores

- es decir, si los predictores correlacionan, se debe **remover** la parte de la variable que correlacinoa con la otra variable

  - ## = Parcialización



---
# Regresión múltiple: más de 1 predictor


.pull-left[
.center[![:scale 60%](../images/ingresoeduc.png)]
.small[
$$\widehat{Ingreso}=b_0+b_1(Educ)$$
]
]

--

.pull-right[
.center[![:scale 60%](../05-regmul1/ingresoeducexp.png)]
.small[
$$\widehat{Ingreso}=b_0+b_1(Educ)+b_2(Int)$$
]]

---
.pull-left-wide[
## Control estadístico
- ¿Qué efecto posee el nivel educacional en ingreso, _controlando por_ inteligencia?
]
.pull-right-narrow[
![](../05-regmul1/ingresoeducexp.png)
]

**Conceptualmente:**
.small[
- aislar el efecto de educación en ingreso, manteniendo la inteligencia _constante_.

- estimar el efecto de educación en ingreso independiente del efecto de la inteligencia

- estimación del efecto de educación en ingreso _ceteris paribus_ (manteniendo el efecto del resto de los predictores constante)
]

---
# Ejemplo

.center[![](../images/paperclasemedia.png)]

.small[Castillo, J., Miranda, D. & Madero, I. (2013) Todos somos de clase media: Sobre el estatus social subjetivo en Chile. _Latin American Research Review_ 48(1) 155-173
]


---
class: inverse

### RESUMEN

- Los coeficientes de regresión (X) no alteran su valor en los modelos en ausencia de correlación entre ellos

- Si hay correlación entre predictores, el valor de los coeficientes de regresión será distinto en modelos simples y en modelos múltiples

- Por ello, en regresión múltiple se habla de coeficientes de regresión **parciales**

---
class: roja, middle, center


# 2. Bases de control y parcialización
---

# Ejemplo (mínimo): Datos

.medium[
```{r echo=FALSE, results='hide'}
pacman::p_load(dplyr,
               corrplot,
               ggplot2,
               scatterplot3d,
               texreg,
              stargazer
)
datos=read.csv("ingedexp.csv", sep=",")
```

```{r}
datos
```
]

---
# Ejemplo: Descriptivos

.medium[
```{r message=FALSE, results='asis'}
stargazer(datos, type = "html", digits=0)
```
]
---
# Ejemplo: correlaciones

.pull-left[.small[
```{r}
cormat=datos %>% select(ingr,educ,intelig) %>% cor()
round(cormat, digits=2)
```
]
]
--

.pull-right[
```{r}
corrplot.mixed(cormat, number.cex=2, tl.cex=1.5)
```
]

---
.pull-left[
**Ingreso <- educación ( $X_1$ )**

```{r echo=FALSE}
ggplot(datos, aes(x=educ, y=ingr)) + geom_point() +
  geom_smooth(method=lm, se=FALSE)
```
]

.pull-right[
**Ingreso <- intelig ( $X_2$ )**

```{r echo=FALSE}
ggplot(datos, aes(x=intelig, y=ingr)) + geom_point() +
  geom_smooth(method=lm, se=FALSE)
```
]

---
# Ejemplo: scatter X1 X2

.pull-left[
```{r echo=FALSE}
ggplot(datos, aes(x=intelig, y=educ)) + geom_point() +
  geom_smooth(method=lm, se=FALSE)
```
]

.pull-right[
.medium[
- presencia de correlación entre predictores

- idea de control estadístico: ¿Cuál es la influencia de educación en ingreso, independiente de la inteligencia?

- **controlando por** inteligencia

]]

---
# Regresión

.small[
```{r}
reg_y_x1=lm(ingr ~ educ, data=datos)
reg_y_x2=lm(ingr ~ intelig, data=datos)
reg_y_x1_x2=lm(ingr ~ educ + intelig , data=datos)
```
]
.small[
```{r echo=FALSE, results='asis'}
htmlreg(list(reg_y_x1,reg_y_x2,reg_y_x1_x2), booktabs = TRUE, dcolumn = TRUE, doctype = FALSE, caption=" ")
```
]

---
class: inverse 
## RESUMEN

- Regresión múltiple: más de un predictor

- No es equivalente a realizar regresiones múltiples por separado con cada predictor y luego simplemente "sumarlas"

- Predictores correlacionados: requiere consideración, ya que de otra manera se estaría sobreestimando el efecto de $X$ en $Y$

---
class: inverse 

## RESUMEN (2)


- Además de ser una corrección estadística, el control se relaciona con preguntas sustantivas basadas en teoría

- El output de regresión múltiple realiza automáticamente el control estadístico vía parcialización de coeficientes.

- ¿Cuál es el cálculo detrás de la parcialización / control estadístico? 

 ... a continuación.


---
class: roja, center, middle

# 3. Demostración parcialización

---
# Antecedentes

- la **parcialización** que permite el control estadístico es un calculo de mediana complejidad

- su complejidad aumenta a medida que aumenta el número de predictores

- por ello, los softwares lo realizan de manera automática, no es necesario realizarlo por cuenta propia

- **PERO**  ... vale la pena demostrarlo, para entender de qué se trata


---
# Parcialización 1

_¿Cómo se despeja la regresión de $Y$ en $X_1$ del efecto de $X_2$?_

.pull-left[
.center[![:scale 65%](../05-regmul1/ingresoeducexp.png)]
]

--

.pull-right[
.center[![:scale 65%](ingeduc_parint.png)]
]

---
class: center middle

# Implica despejar $X_1$ de su correlación con $X_2$, o **parcializar** $X_1$ de $X_2$

---
.pull-left[
# Parcialización 3

.medium[
¿Como obtenemos una variable $X_1$ parcializada de $X_2$?
]

.center[
![:scale 100%](../images/partial2.png)]
]
--

.pull-right[

<br>
<br>

.medium[
- Pensemos en que $X_1$ parcializada (de $X_2$ ) es todo lo de $X_1$ (varianza) que no tiene que ver con $X_2$

- En otras palabras, en un modelo donde $X_1$ es la variable dependiente y $X_2$ la independiente, $X_1$ parcializada equivale al **residuo** de esta regresión
]
]
---
# Parcialización 4

Por lo tanto, para **demostrar** el concepto de parcialización en el ejemplo, los pasos son:

1. Regresión entre predictores

2. Obtención del residuo de la regresión

3. Regresión de $Y$ en el residuo (=la variable parcializada)

---
# Parcialización 5

.pull-left[
**1.Regresión entre predictores **

```{r echo=TRUE}
reg_x1_x2=lm(educ ~ intelig, data=datos)
coef(reg_x1_x2)
```
]

.pull-right[
.medium[
Por lo tanto, tenemos que nuestro modelo de regresión entre predictores, con educación como dependiente es:

$$\widehat{educacion}=2.66+0.541_{inteligencia}$$]]

---
class: center, middle

$$\widehat{educacion}=2.66+0.541_{inteligencia}$$

```{r echo=FALSE}
ggplot(datos, aes(x=intelig, y=educ)) + geom_point() +
  geom_smooth(method=lm, se=FALSE)
```

---
# Parcialización 6

**2.Obtención de residuo (valor estimado - observado)**

.small[
```{r}
x1_fit_x2=fitted.values(reg_x1_x2)
resx1_2=residuals(reg_x1_x2)
datos=cbind(datos, x1_fit_x2,resx1_2); datos
```
]
---

Ejemplo caso 1: inteligencia=1

Estimando valor predicho según nuestro modelo:

$$\widehat{educacion}=2.66+0.541*1=3.196$$

- **3.196** es el valor predicho de educación para una persona de inteligencia = 1

- sabemos que el valor observado de educación para el caso 1 es igual a **2**

- por lo tanto, el residuo para este caso (valor observado - valor estimado $=2-3.196=-1.2$


---
### Parcialización 7: Regresión de Y en variable $X_1$ parcializada = $X_{1.2}$

```{r}
regy_resx1_2=lm(datos$ingr ~ resx1_2)
```
.small[
```{r echo=FALSE, results='asis'}
htmlreg(list(reg_y_x1,reg_y_x2,reg_y_x1_x2,regy_resx1_2), booktabs = TRUE, dcolumn = TRUE, doctype = FALSE, caption=" ")
```
]


---
# Parcialización 8

Ahora, lo mismo pero parcializando inteligencia ( $X_2$ ) de educación ( $X_1$ )


**1.Regresión de $X_2$ (inteligencia) en $X_1$ (educación)**

```{r}
reg_x2_x1=lm(intelig ~ educ, data=datos)
```

**2. Obtención del residuo de la regresión (inteligencia parcializada de educación)**

```{r}
resx2_1=residuals(reg_x2_x1)
```

**3. Regresión de $Y$ (ingreso) en la variable parcializada $X_{2.1}$**

```{r}
regy_resx2_1=lm(datos$ingr ~ resx2_1)
regfin=lm(datos$ingr ~ resx1_2 + resx2_1 )
```

---
### Comparación final modelos

.small[
```{r echo=FALSE, results='asis'}
htmlreg(list(reg_y_x1,reg_y_x2,reg_y_x1_x2,regy_resx1_2,regy_resx2_1), booktabs = TRUE, dcolumn = TRUE, doctype = FALSE, caption=" ")
```
]


---
# Comparando pendientes

.pull-left[
**Ingreso <- educ $X_1$**

```{r echo=FALSE}
ggplot(datos, aes(x=educ, y=ingr)) + geom_point() +
  geom_smooth(method=lm, se=FALSE)
```
]

.pull-right[
**Ingreso <- educ.parcial $X_{1.2}$**

```{r echo=FALSE}
ggplot(datos, aes(x=resx2_1, y=ingr)) + geom_point() +
  geom_smooth(method=lm, se=FALSE)
```
]

---
## Matriz de correlaciones con variables parcializadas

```{r results='hide'}
datcor= cbind(datos, resx2_1)
datcor <- datcor %>% select(-x1_fit_x2,-ID)
cormat2= cor(datcor)
round(cormat2, digits=2)
```

.center[
```{r}
corrplot.mixed(cormat2, number.cex=2, tl.cex=1.5)
```
]

---
# Fórmulas directas de regresores parciales:

$$b_1=\biggl(\frac{s_y}{s_1}\biggr)\biggl(\frac{r_{y1}-r_{y2}r_{12}}{1-r^2_{12}}\biggr)$$

$$b_2=\biggl(\frac{s_y}{s_2}\biggr)\biggl(\frac{r_{y2}-r_{y1}r_{12}}{1-r^2_{12}}\biggr)$$

---
class: roja

## RESUMEN

- El control estadístico es central en regresión múltiple

--

- Pregunta: ¿Es la relación entre _X_ e _Y_ _realmente_ debida a _X_, o hay otras variables que podrían dar cuenta de esta relación?

--

- El control se implementa agregando predictores en el modelo que por razones teóricas se presume pueden afectar la relación del regresor principal en _Y_

---
class: roja

## RESUMEN (2)

- En términos técnicos, el control estadístico opera mediante *parcialización*: los predictores se parcializan mutuamente, generando coeficientes de regresión parciales.

--

- El regresor parcial entonces es un regresor ajustado por la presencia de otro(s) regresore(s)

--

- Por lo tanto, ahora los betas de regresión se pueden sumar apropiadamente, sin distorsionar la predicción


---
class: inverse, middle, center

## PROXIMA CLASE:

# INFERENCIA EN REGRESIÓN


---
class: front

.pull-left[
# Estadística Multivariada
## Juan Carlos Castillo
## Sociología FACSO - UChile
## 1er Sem 2020
## [multivariada.netlify.com](https://multivariada.netlify.com)
]


.pull-right[
.right[
<br>
![:scale 80%](https://multivariada.netlify.com/img/hex_multiva.png)
]

]


